{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext nb_black\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-U8BBOJI:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x21ef8376b48>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import isfile, join\n",
    "\n",
    "loc = os.path.abspath(\"\")\n",
    "data_loc = f\"{loc}/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: https://www.kaggle.com/ntnu-testimon/paysim1/data (Randomly sampled 10% of the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\n",
    "    f\"{data_loc}Synthetic Financial Data.csv\", inferSchema=True, header=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- step: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- nameOrig: string (nullable = true)\n",
      " |-- oldbalanceOrg: double (nullable = true)\n",
      " |-- newbalanceOrig: double (nullable = true)\n",
      " |-- nameDest: string (nullable = true)\n",
      " |-- oldbalanceDest: double (nullable = true)\n",
      " |-- newbalanceDest: double (nullable = true)\n",
      " |-- isFraud: integer (nullable = true)\n",
      " |-- isFlaggedFraud: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "|step|   type| amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|\n",
      "+----+-------+-------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "|   1|PAYMENT|9839.64|C1231006815|     170136.0|     160296.36|M1979787155|           0.0|           0.0|      0|             0|\n",
      "|   1|PAYMENT|1864.28|C1666544295|      21249.0|      19384.72|M2044282225|           0.0|           0.0|      0|             0|\n",
      "+----+-------+-------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(\"type\", \"amount\", \"oldbalanceOrg\", \"newbalanceOrig\", \"isFraud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+--------------+-------+\n",
      "|   type| amount|oldbalanceOrg|newbalanceOrig|isFraud|\n",
      "+-------+-------+-------------+--------------+-------+\n",
      "|PAYMENT|9839.64|     170136.0|     160296.36|      0|\n",
      "|PAYMENT|1864.28|      21249.0|      19384.72|      0|\n",
      "+-------+-------+-------------+--------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3], seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set length: 4452488 records\n",
      "Test set length: 1910132 records\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train set length: {train.count()} records\")\n",
    "print(f\"Test set length: {test.count()} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------------+--------------+-------+\n",
      "|   type|amount|oldbalanceOrg|newbalanceOrig|isFraud|\n",
      "+-------+------+-------------+--------------+-------+\n",
      "|CASH_IN|  5.44|          0.0|          5.44|      0|\n",
      "|CASH_IN|  5.66|   5061561.06|    5061566.72|      0|\n",
      "+-------+------+-------------+--------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dtypes\n",
    "In this dataset, any column of type string is treated as a categorical feature, but sometimes we might have numeric features we want treated as categorical or vice versa. Weâ€™ll need to carefully identify which columns are numeric and which are categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('type', 'string'),\n",
       " ('amount', 'double'),\n",
       " ('oldbalanceOrg', 'double'),\n",
       " ('newbalanceOrig', 'double'),\n",
       " ('isFraud', 'int')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "catCols = [x for (x, dataType) in train.dtypes if dataType == \"string\"]\n",
    "numCols = [\n",
    "    x for (x, dataType) in train.dtypes if ((dataType == \"double\") & (x != \"isFraud\"))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amount', 'oldbalanceOrg', 'newbalanceOrig']\n",
      "['type']\n"
     ]
    }
   ],
   "source": [
    "print(numCols)\n",
    "print(catCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding\n",
    "\n",
    "StringIndexer:\n",
    "Converts a single feature to an index feature.\n",
    "http://spark.apache.org/docs/latest/ml-features#stringindexer\n",
    "\n",
    "\n",
    "OneHotEncoder:\n",
    "http://spark.apache.org/docs/latest/ml-features#onehotencoder\n",
    "\n",
    "For more info: http://spark.apache.org/docs/latest/ml-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|count(type)|\n",
      "+-----------+\n",
      "|          5|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.agg(F.countDistinct(\"type\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|    type|  count|\n",
      "+--------+-------+\n",
      "|TRANSFER| 373168|\n",
      "| CASH_IN| 978910|\n",
      "|CASH_OUT|1566234|\n",
      "| PAYMENT|1505050|\n",
      "|   DEBIT|  29126|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.groupBy(\"type\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (\n",
    "    OneHotEncoder,\n",
    "    StringIndexer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_indexer = [\n",
    "    StringIndexer(inputCol=x, outputCol=x + \"_StringIndexer\", handleInvalid=\"skip\")\n",
    "    for x in catCols\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_68ef70072b3c]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder = [\n",
    "    OneHotEncoder(\n",
    "        inputCols=[f\"{x}_StringIndexer\" for x in catCols],\n",
    "        outputCols=[f\"{x}_OneHotEncoder\" for x in catCols],\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OneHotEncoder_fe449ef85264]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector assembling\n",
    "\n",
    "VectorAssembler:\n",
    "Combines the values of input columns into a single vector.\n",
    "http://spark.apache.org/docs/latest/ml-features#vectorassembler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "assemblerInput = [x for x in numCols]\n",
    "assemblerInput += [f\"{x}_OneHotEncoder\" for x in catCols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amount', 'oldbalanceOrg', 'newbalanceOrig', 'type_OneHotEncoder']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assemblerInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_assembler = VectorAssembler(\n",
    "    inputCols=assemblerInput, outputCol=\"VectorAssembler_features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []\n",
    "stages += string_indexer\n",
    "stages += one_hot_encoder\n",
    "stages += [vector_assembler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_68ef70072b3c,\n",
       " OneHotEncoder_fe449ef85264,\n",
       " VectorAssembler_8be0ff2e4160]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline().setStages(stages)\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "pp_df = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------------+--------------+--------------------------------------------------+\n",
      "|type   |amount|oldbalanceOrg|newbalanceOrig|VectorAssembler_features                          |\n",
      "+-------+------+-------------+--------------+--------------------------------------------------+\n",
      "|CASH_IN|8.44  |39384.0      |39392.44      |[8.44,39384.0,39392.44,0.0,0.0,1.0,0.0]           |\n",
      "|CASH_IN|12.79 |601743.0     |601755.79     |[12.79,601743.0,601755.79,0.0,0.0,1.0,0.0]        |\n",
      "|CASH_IN|14.4  |1.143460813E7|1.143462253E7 |[14.4,1.143460813E7,1.143462253E7,0.0,0.0,1.0,0.0]|\n",
      "|CASH_IN|15.52 |4368030.06   |4368045.59    |[15.52,4368030.06,4368045.59,0.0,0.0,1.0,0.0]     |\n",
      "|CASH_IN|15.59 |1.64294897E7 |1.642950528E7 |[15.59,1.64294897E7,1.642950528E7,0.0,0.0,1.0,0.0]|\n",
      "|CASH_IN|18.53 |951352.78    |951371.31     |[18.53,951352.78,951371.31,0.0,0.0,1.0,0.0]       |\n",
      "|CASH_IN|21.15 |2729078.29   |2729099.44    |[21.15,2729078.29,2729099.44,0.0,0.0,1.0,0.0]     |\n",
      "|CASH_IN|22.67 |405940.0     |405962.67     |[22.67,405940.0,405962.67,0.0,0.0,1.0,0.0]        |\n",
      "|CASH_IN|25.01 |3077402.04   |3077427.04    |[25.01,3077402.04,3077427.04,0.0,0.0,1.0,0.0]     |\n",
      "|CASH_IN|34.05 |90282.0      |90316.05      |[34.05,90282.0,90316.05,0.0,0.0,1.0,0.0]          |\n",
      "|CASH_IN|35.47 |3796691.21   |3796726.68    |[35.47,3796691.21,3796726.68,0.0,0.0,1.0,0.0]     |\n",
      "|CASH_IN|37.11 |1452790.24   |1452827.35    |[37.11,1452790.24,1452827.35,0.0,0.0,1.0,0.0]     |\n",
      "|CASH_IN|38.77 |6210013.72   |6210052.49    |[38.77,6210013.72,6210052.49,0.0,0.0,1.0,0.0]     |\n",
      "|CASH_IN|45.47 |4634070.59   |4634116.06    |[45.47,4634070.59,4634116.06,0.0,0.0,1.0,0.0]     |\n",
      "|CASH_IN|49.27 |1681309.13   |1681358.4     |[49.27,1681309.13,1681358.4,0.0,0.0,1.0,0.0]      |\n",
      "|CASH_IN|57.98 |1290788.6    |1290846.57    |[57.98,1290788.6,1290846.57,0.0,0.0,1.0,0.0]      |\n",
      "|CASH_IN|57.98 |9021204.76   |9021262.74    |[57.98,9021204.76,9021262.74,0.0,0.0,1.0,0.0]     |\n",
      "|CASH_IN|59.23 |20841.0      |20900.23      |[59.23,20841.0,20900.23,0.0,0.0,1.0,0.0]          |\n",
      "|CASH_IN|71.03 |15197.0      |15268.03      |[71.03,15197.0,15268.03,0.0,0.0,1.0,0.0]          |\n",
      "|CASH_IN|72.42 |6183.0       |6255.42       |[72.42,6183.0,6255.42,0.0,0.0,1.0,0.0]            |\n",
      "+-------+------+-------------+--------------+--------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pp_df.select(\n",
    "    \"type\", \"amount\", \"oldbalanceOrg\", \"newbalanceOrig\", \"VectorAssembler_features\",\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pp_df.select(\n",
    "    F.col(\"VectorAssembler_features\").alias(\"features\"),\n",
    "    F.col(\"isFraud\").alias(\"label\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+-----+\n",
      "|features                                          |label|\n",
      "+--------------------------------------------------+-----+\n",
      "|[8.44,39384.0,39392.44,0.0,0.0,1.0,0.0]           |0    |\n",
      "|[12.79,601743.0,601755.79,0.0,0.0,1.0,0.0]        |0    |\n",
      "|[14.4,1.143460813E7,1.143462253E7,0.0,0.0,1.0,0.0]|0    |\n",
      "|[15.52,4368030.06,4368045.59,0.0,0.0,1.0,0.0]     |0    |\n",
      "|[15.59,1.64294897E7,1.642950528E7,0.0,0.0,1.0,0.0]|0    |\n",
      "+--------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression().fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9908572890158174"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|            recall|           precision|\n",
      "+------------------+--------------------+\n",
      "|               0.0| 0.10545530405757797|\n",
      "|0.8410981697171381| 0.10545530405757797|\n",
      "|0.9097337770382695| 0.05730981892508058|\n",
      "|0.9525790349417638| 0.04007069240056694|\n",
      "|0.9825291181364393|  0.0310234317537039|\n",
      "|0.9950083194675541|0.025141631893715644|\n",
      "|0.9950083194675541|0.020954516784638028|\n",
      "|0.9950083194675541| 0.01796011532916867|\n",
      "|0.9950083194675541|0.015715853169779834|\n",
      "|0.9954242928452579|0.013975436403880184|\n",
      "|0.9954242928452579|0.012577261068830677|\n",
      "|0.9954242928452579|0.011433185381958214|\n",
      "|0.9954242928452579|0.010479894193797024|\n",
      "|0.9954242928452579|0.009673416094332987|\n",
      "|0.9954242928452579| 0.00898276645182602|\n",
      "|0.9954242928452579| 0.00838369506192303|\n",
      "|0.9954242928452579|0.008013636241862458|\n",
      "|0.9958402662229617| 0.00753669178897263|\n",
      "|0.9962562396006656|0.007113452890824092|\n",
      "|0.9962562396006656|0.006732653227597413|\n",
      "+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.summary.pr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
